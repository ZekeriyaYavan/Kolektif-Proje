# -*- coding: utf-8 -*-
"""kolektif_proje_(7)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ruzxbhjJ4tCqvf6OQVJvxJPC6t9PnYaY
"""

# ==========================================
# GOOGLE DRIVE BAƒûLAMA (ZORLA / FORCE)
# ==========================================
from google.colab import drive
import os

print("üîó Google Drive baƒülanƒ±yor (Force Mode)...")

# force_remount=True ekledik, bu sayede "i√ßi dolu" hatasƒ±nƒ± ezip ge√ßer.
drive.mount('/content/drive', force_remount=True)

# Proje ana klas√∂r√º
PROJECT_DIR = "/content/drive/MyDrive/Proje_Distill"
os.makedirs(PROJECT_DIR, exist_ok=True)

print("üìÅ Proje klas√∂r√º:", PROJECT_DIR)
print("‚úÖ Drive ba≈üarƒ±yla baƒülandƒ±!")

# ==========================================
# TORCH FIX (libtorch_global_deps.so hatasƒ±)
# ==========================================
!pip uninstall -y -q torch torchvision torchaudio

# Colab GPU i√ßin stabil torch kurulumu
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

print("‚úÖ Torch yeniden kuruldu. ≈ûimdi Runtime Restart yapmalƒ±sƒ±n.")

import torch

print("torch:", torch.__version__)
print("cuda available:", torch.cuda.is_available())

import os

# Proje klas√∂r√º (Drive i√ßinde)
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"

# Dosyalar
INPUT_FILE = os.path.join(BASE_PATH, "train_2000.jsonl")
TEST_FILE  = os.path.join(BASE_PATH, "test_500.jsonl")

print("üìÅ BASE_PATH:", BASE_PATH)
print("üìÑ INPUT_FILE:", INPUT_FILE)

# Klas√∂r yoksa olu≈ütur
os.makedirs(BASE_PATH, exist_ok=True)

# Dosya var mƒ± kontrol
print("‚úÖ train_2000.jsonl var mƒ±?", os.path.exists(INPUT_FILE))
print("‚úÖ test_500.jsonl var mƒ±?", os.path.exists(TEST_FILE))

# ==========================================
# H√úCRE 2: VERƒ∞ HAZIRLIƒûI (GSM8K - T√úRK√áE)
# ==========================================
import json
import random
import os
from datasets import load_dataset

# Eƒüer √∂nceki h√ºcrede PROJECT_DIR tanƒ±mlƒ±ysa bunu kullan:
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
os.makedirs(BASE_PATH, exist_ok=True)

# Sabit Seed (Her denemede aynƒ± sorularƒ±n gelmesi i√ßin)
SEED = 42
random.seed(SEED)

print("üì• GSM8K TR (ytu-ce-cosmos) veri seti indiriliyor...")

# 1) T√ºrk√ße Veri Setini Y√ºkle
try:
    dataset = load_dataset("ytu-ce-cosmos/gsm8k_tr", split="train")
    print(f"‚úÖ Veri indirildi. Toplam Soru Sayƒ±sƒ±: {len(dataset)}")

    # Veri setinin formatƒ±nƒ± kontrol edelim
    print(f"üîç ƒ∞lk veri √∂rneƒüi: {dataset[0]}")

    all_data = list(dataset)

except Exception as e:
    print(f"‚ùå Veri seti hatasƒ±: {e}")
    all_data = []

if all_data:
    # 2) Verileri Karƒ±≈ütƒ±r
    indices = list(range(len(all_data)))
    random.shuffle(indices)

    # 3) Eƒüitim (2000) ve Test (500) Olarak Ayƒ±r
    TRAIN_SIZE = 2000
    TEST_SIZE = 500

    train_data = [all_data[i] for i in indices[:TRAIN_SIZE]]
    test_data  = [all_data[i] for i in indices[TRAIN_SIZE:TRAIN_SIZE + TEST_SIZE]]

    # 4) Kaydetme Fonksiyonu
    def save_jsonl(data, filename):
        path = os.path.join(BASE_PATH, filename)
        written = 0

        with open(path, "w", encoding="utf-8") as f:
            for item in data:
                q_text = item.get("question")
                a_text = item.get("answer")

                if not q_text or not a_text:
                    continue

                record = {
                    "question": q_text,
                    "answer": a_text
                }

                f.write(json.dumps(record, ensure_ascii=False) + "\n")
                written += 1

        print(f"üíæ Kaydedildi: {path} | satƒ±r: {written}")

    # Dosyalarƒ± Olu≈ütur
    save_jsonl(train_data, "train_2000.jsonl")
    save_jsonl(test_data, "test_500.jsonl")

    print(f"\n‚úÖ Veri Setleri Hazƒ±r! (Train: {len(train_data)}, Test: {len(test_data)})")
else:
    print("‚ö†Ô∏è Veri seti bo≈ü olduƒüu i√ßin i≈ülem yapƒ±lamadƒ±.")

# ==========================================
# 1. ADIM: K√úT√úPHANE G√úNCELLEME (ZORUNLU)
# ==========================================
print("üîÑ K√ºt√ºphaneler g√ºncelleniyor (GitHub S√ºr√ºm√º)...")

# Standart pip install yetmez, en son geli≈ütirici s√ºr√ºm√ºn√º almamƒ±z lazƒ±m
!pip install -U git+https://github.com/huggingface/transformers.git
!pip install -U accelerate bitsandbytes sentencepiece

print("‚úÖ G√ºncelleme tamamlandƒ±. ≈ûƒ∞MDƒ∞ RUNTIME RESTART YAPMALISIN!")

!pip install --upgrade huggingface_hub transformers

# ==========================================
# H√úCRE 1: SETUP & FONKSƒ∞YONLAR (TURBO BATCH)
# ==========================================
import torch, gc, json, os, re, time
from transformers import AutoTokenizer, AutoModelForCausalLM

BATCH_SIZE = 8
MAX_NEW_TOKENS = 450
RETRY_TOKENS = 64

BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
INPUT_FILE = os.path.join(BASE_PATH, "train_2000.jsonl")
os.makedirs(BASE_PATH, exist_ok=True)

def extract_final_number(text: str):
    if not text: return None
    m = re.search(r"Final\s*Answer\s*:?\s*(.*)", text, flags=re.IGNORECASE)
    if m:
        tail = m.group(1).strip()
        m2 = re.search(r"([-+]?\d+(?:\.\d+)?)", tail)
        if m2: return m2.group(1)

    m = re.search(r"(Cevap|Sonu√ß)\s*:?\s*(.*)", text, flags=re.IGNORECASE)
    if m:
        tail = m.group(2).strip()
        m2 = re.search(r"([-+]?\d+(?:\.\d+)?)", tail)
        if m2: return m2.group(1)
    return None

def fallback_last_number(text: str):
    nums = re.findall(r"([-+]?\d+(?:\.\d+)?)", text)
    if not nums: return None
    return nums[-1]

def fix_format(answer: str):
    if answer is None: return None, None
    answer = answer.strip()

    final = extract_final_number(answer)
    if final is None:
        final = fallback_last_number(answer)
    if final is None:
        return None, None

    m2 = re.search(r"<think>(.*?)</think>", answer, flags=re.DOTALL | re.IGNORECASE)
    if m2:
        think = m2.group(1).strip()
    else:
        # think yoksa: t√ºm metni d√º≈ü√ºnce kabul et
        think = answer.strip()

    think = think.replace("<think>", "").replace("</think>", "").strip()
    think = re.sub(r"\n{3,}", "\n\n", think)

    formatted = f"<think>\n{think}\n</think>\nFinal Answer: {final}"
    return formatted, final

def build_chat_messages(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    user_msg = (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece formatƒ± g√∂stermek i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama! Kendi bulduƒüun sonucu yaz!\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}"
    )
    return [{"role": "user", "content": user_msg}]

@torch.no_grad()
def run_teacher_specific(teacher_name, model_id):
    output_path = os.path.join(BASE_PATH, f"distill_{teacher_name}.jsonl")
    print(f"\nüöÄ BA≈ûLIYOR (BATCH MODU): {teacher_name}")
    print(f"üíæ Output: {output_path}")

    if not os.path.exists(INPUT_FILE):
        print("‚ùå INPUT yok:", INPUT_FILE)
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        all_questions = [json.loads(line)["question"] for line in f]

    start_index = 0
    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as f:
            start_index = sum(1 for _ in f)
        print(f"   ‚ö†Ô∏è Resume: {start_index}/{len(all_questions)}")

    if start_index >= len(all_questions):
        print("   ‚úÖ Bu model i√ßin i≈ülem tamamlanmƒ±≈ü.")
        return

    print("   -> Model y√ºkleniyor...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    t0 = time.time()
    saved, skipped = 0, 0

    with open(output_path, "a", encoding="utf-8") as f_out:
        for i in range(start_index, len(all_questions), BATCH_SIZE):
            batch_q = all_questions[i:i+BATCH_SIZE]

            batch_prompts = []
            for q in batch_q:
                msgs = build_chat_messages(q)
                if hasattr(tokenizer, "apply_chat_template"):
                    txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
                else:
                    txt = msgs[0]["content"]
                batch_prompts.append(txt)

            inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True).to(model.device)

            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
            except Exception as e:
                print(f"‚ùå Batch generate hatasƒ±: {e}")
                continue

            # üî• EN KRƒ∞Tƒ∞K D√úZELTME:
            # prompt'u kesip sadece "yeni √ºretilen kƒ±smƒ±" alƒ±yoruz
            prompt_len = inputs["input_ids"].shape[1]
            gen_only = outputs[:, prompt_len:]
            generated_texts = tokenizer.batch_decode(gen_only, skip_special_tokens=True)

            for j, raw_text in enumerate(generated_texts):
                q_curr = batch_q[j]
                response, final = fix_format(raw_text)

                if response is None:
                    # Retry tekil
                    retry_text = build_retry_prompt(q_curr)
                    inp2 = tokenizer(retry_text, return_tensors="pt").to(model.device)
                    out2 = model.generate(
                        **inp2,
                        max_new_tokens=RETRY_TOKENS,
                        do_sample=False,
                        pad_token_id=tokenizer.pad_token_id
                    )
                    raw2 = tokenizer.decode(out2[0][inp2.input_ids.shape[1]:], skip_special_tokens=True)
                    response, final = fix_format(raw2)

                    if response is None:
                        skipped += 1
                        continue

                record = {
                    "teacher": teacher_name,
                    "model_id": model_id,
                    "question": q_curr,
                    "answer": response
                }
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                f_out.flush()  # üî• garanti yaz

                saved += 1

            # log
            if (i + BATCH_SIZE) % 20 < BATCH_SIZE:
                elapsed = (time.time() - t0) / 60
                speed = saved / elapsed if elapsed > 0 else 0
                print(f"   ‚úÖ {i+len(batch_q)}/{len(all_questions)} | saved={start_index+saved} | skipped={skipped} | Hƒ±z: {speed:.1f} soru/dk")

    del model, tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    print(f"üéâ {teacher_name} Bƒ∞TTƒ∞!")

# ==========================================
# TEACHER 1: COSMOS T1
# ==========================================

run_teacher_specific(
    teacher_name="Cosmos_T1",
    model_id="ytu-ce-cosmos/Turkish-Gemma-9b-T1"
)

import gc, torch
gc.collect()
torch.cuda.empty_cache()
print("üßπ GPU temizlendi, sƒ±radakine hazƒ±r.")

# ==========================================
# TEACHER 2: QWEN TURKISH
# ==========================================

# 1. Modeli √áalƒ±≈ütƒ±r
run_teacher_specific(
    teacher_name="Qwen_TR",
    model_id="hosasmek/qwen2.5-7b-instruct-reasoning-tr"
)

# 2. Temizlik Yap (Biti≈üte)
import gc, torch
gc.collect()
torch.cuda.empty_cache()
print("üßπ Qwen bitti, GPU temizlendi. Sƒ±radakine ge√ßebilirsiniz.")

# ==========================================
# H√úCRE 2: KESƒ∞N √á√ñZ√úM (GenerationMixin Patch)
# ==========================================

from transformers.generation.utils import GenerationMixin
import gc, torch

def run_teacher_versioned(teacher_name, model_id, temp, top_p, max_new_tokens, do_sample=True):
    """
    H√ºcre 1'e dokunmadan, GenerationMixin √ºzerindeki generate fonksiyonunu
    ge√ßici olarak deƒüi≈ütirir. Bu y√∂ntem en garantisidir.
    """

    # 1. Asƒ±l generate fonksiyonunun ger√ßek sahibi GenerationMixin'dir.
    original_generate = GenerationMixin.generate

    # 2. Kendi ayarlarƒ±mƒ±zƒ± zorlayan fonksiyonu yazƒ±yoruz
    def patched_generate(self, *args, **kwargs):
        # Parametreleri eziyoruz (Override)
        kwargs["do_sample"] = do_sample
        kwargs["temperature"] = temp
        kwargs["top_p"] = top_p
        kwargs["max_new_tokens"] = max_new_tokens

        # Orijinal fonksiyonu yeni ayarlarla √ßaƒüƒ±rƒ±yoruz
        return original_generate(self, *args, **kwargs)

    # 3. YAMALAMA: Artƒ±k t√ºm modeller bu fonksiyonu kullanacak
    GenerationMixin.generate = patched_generate

    print(f"\nüîß {teacher_name} AKTƒ∞F (GenerationMixin Patching):")
    print(f"   do_sample={do_sample} | temp={temp} | top_p={top_p} | max_new_tokens={max_new_tokens}")

    try:
        # H√ºcre 1'deki fonksiyonu √ßalƒ±≈ütƒ±r
        # (Bu fonksiyon model.generate √ßaƒüƒ±rdƒ±ƒüƒ±nda aslƒ±nda patched_generate √ßalƒ±≈üacak)
        run_teacher_specific(teacher_name=teacher_name, model_id=model_id)
    except Exception as e:
        print(f"‚ùå Hata: {e}")
    finally:
        # 4. TEMƒ∞ZLƒ∞K: Her ≈üeyi eski haline getir (√áok √∂nemli!)
        GenerationMixin.generate = original_generate
        print("üîß generate fonksiyonu orijinal haline d√∂nd√º.")
        gc.collect()
        torch.cuda.empty_cache()

# --- V2 ƒ∞√áƒ∞N √áALI≈ûTIR ---
run_teacher_versioned(
    teacher_name="Cosmos_T1_v2",
    model_id="ytu-ce-cosmos/Turkish-Gemma-9b-T1",
    temp=0.6,
    top_p=0.9,
    max_new_tokens=256,
    do_sample=True
)

# ==========================================
# H√úCRE 2: KESƒ∞N √á√ñZ√úM (GenerationMixin Patch & Eksik Fonksiyon)
# ==========================================

from transformers.generation.utils import GenerationMixin
import gc, torch
import sys  # Gerekirse mod√ºl seviyesinde eklemek i√ßin

# 1. Eksik Fonksiyonu Tanƒ±mlƒ±yoruz
def build_retry_prompt(question):
    return f"Soru: {question}\n\n√ñnceki cevap formatƒ± hatalƒ±ydƒ±. L√ºtfen kurallara tam uyarak tekrar cevapla."

# 2. Fonksiyonu Global Alana Ekliyoruz (Bu kritik!)
# B√∂ylece H√ºcre 1'deki kod bu ismi aradƒ±ƒüƒ±nda bulabilecek.
globals()['build_retry_prompt'] = build_retry_prompt

def run_teacher_versioned(teacher_name, model_id, temp, top_p, max_new_tokens, do_sample=True):
    """
    H√ºcre 1'e dokunmadan, GenerationMixin √ºzerindeki generate fonksiyonunu
    ge√ßici olarak deƒüi≈ütirir. Bu y√∂ntem en garantisidir.
    """

    # Asƒ±l generate fonksiyonunun ger√ßek sahibi GenerationMixin'dir.
    original_generate = GenerationMixin.generate

    # Kendi ayarlarƒ±mƒ±zƒ± zorlayan fonksiyonu yazƒ±yoruz
    def patched_generate(self, *args, **kwargs):
        # Parametreleri eziyoruz (Override)
        kwargs["do_sample"] = do_sample
        kwargs["temperature"] = temp
        kwargs["top_p"] = top_p
        kwargs["max_new_tokens"] = max_new_tokens

        # Orijinal fonksiyonu yeni ayarlarla √ßaƒüƒ±rƒ±yoruz
        return original_generate(self, *args, **kwargs)

    # YAMALAMA: Artƒ±k t√ºm modeller bu fonksiyonu kullanacak
    GenerationMixin.generate = patched_generate

    print(f"\nüîß {teacher_name} AKTƒ∞F (GenerationMixin Patching):")
    print(f"   do_sample={do_sample} | temp={temp} | top_p={top_p} | max_new_tokens={max_new_tokens}")

    try:
        # H√ºcre 1'deki fonksiyonu √ßalƒ±≈ütƒ±r
        run_teacher_specific(teacher_name=teacher_name, model_id=model_id)
    except Exception as e:
        print(f"‚ùå Hata: {e}")
    finally:
        # TEMƒ∞ZLƒ∞K: Her ≈üeyi eski haline getir (√áok √∂nemli!)
        GenerationMixin.generate = original_generate
        print("üîß generate fonksiyonu orijinal haline d√∂nd√º.")
        gc.collect()
        torch.cuda.empty_cache()


# --- V3 ƒ∞√áƒ∞N √áALI≈ûTIR (KA≈ûƒ∞F MODU) ---
run_teacher_versioned(
    teacher_name="Cosmos_T1_v3",
    model_id="ytu-ce-cosmos/Turkish-Gemma-9b-T1",
    temp=0.8,
    top_p=0.95,
    max_new_tokens=256,
    do_sample=True
)

# ==========================================
# H√úCRE 2: KESƒ∞N √á√ñZ√úM (GenerationMixin Patch & Eksik Fonksiyon)
# ==========================================

from transformers.generation.utils import GenerationMixin
import gc, torch
import sys  # Gerekirse mod√ºl seviyesinde eklemek i√ßin

# 1. Eksik Fonksiyonu Tanƒ±mlƒ±yoruz
def build_retry_prompt(question):
    return f"Soru: {question}\n\n√ñnceki cevap formatƒ± hatalƒ±ydƒ±. L√ºtfen kurallara tam uyarak tekrar cevapla."

# 2. Fonksiyonu Global Alana Ekliyoruz (Bu kritik!)
# B√∂ylece H√ºcre 1'deki kod bu ismi aradƒ±ƒüƒ±nda bulabilecek.
globals()['build_retry_prompt'] = build_retry_prompt

def run_teacher_versioned(teacher_name, model_id, temp, top_p, max_new_tokens, do_sample=True):
    """
    H√ºcre 1'e dokunmadan, GenerationMixin √ºzerindeki generate fonksiyonunu
    ge√ßici olarak deƒüi≈ütirir. Bu y√∂ntem en garantisidir.
    """

    # Asƒ±l generate fonksiyonunun ger√ßek sahibi GenerationMixin'dir.
    original_generate = GenerationMixin.generate

    # Kendi ayarlarƒ±mƒ±zƒ± zorlayan fonksiyonu yazƒ±yoruz
    def patched_generate(self, *args, **kwargs):
        # Parametreleri eziyoruz (Override)
        kwargs["do_sample"] = do_sample
        kwargs["temperature"] = temp
        kwargs["top_p"] = top_p
        kwargs["max_new_tokens"] = max_new_tokens

        # Orijinal fonksiyonu yeni ayarlarla √ßaƒüƒ±rƒ±yoruz
        return original_generate(self, *args, **kwargs)

    # YAMALAMA: Artƒ±k t√ºm modeller bu fonksiyonu kullanacak
    GenerationMixin.generate = patched_generate

    print(f"\nüîß {teacher_name} AKTƒ∞F (GenerationMixin Patching):")
    print(f"   do_sample={do_sample} | temp={temp} | top_p={top_p} | max_new_tokens={max_new_tokens}")

    try:
        # H√ºcre 1'deki fonksiyonu √ßalƒ±≈ütƒ±r
        run_teacher_specific(teacher_name=teacher_name, model_id=model_id)
    except Exception as e:
        print(f"‚ùå Hata: {e}")
    finally:
        # TEMƒ∞ZLƒ∞K: Her ≈üeyi eski haline getir (√áok √∂nemli!)
        GenerationMixin.generate = original_generate
        print("üîß generate fonksiyonu orijinal haline d√∂nd√º.")
        gc.collect()
        torch.cuda.empty_cache()


# --- V3 ƒ∞√áƒ∞N √áALI≈ûTIR (KA≈ûƒ∞F MODU) ---
run_teacher_versioned(
    teacher_name="Cosmos_T1_v3",
    model_id="ytu-ce-cosmos/Turkish-Gemma-9b-T1",
    temp=0.6,
    top_p=0.9,
    max_new_tokens=128,
    do_sample=True
)

import os, torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
DATA_PATH = os.path.join(BASE_PATH, "distill_Cosmos_T1_v3.jsonl")
OUT_DIR   = os.path.join(BASE_PATH, "adapter_qwen_student")

STUDENT_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"

dataset = load_dataset("json", data_files=DATA_PATH, split="train")

tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"   # üî• Qwen i√ßin √∂nemli

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","mlp.up_proj","mlp.down_proj","mlp.gate_proj"]
)

model = get_peft_model(model, lora_config)

MAX_LEN = 1024

def build_prompt(question):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è √ñrneƒüi kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def preprocess(ex):
    q = ex["question"].strip()
    a = ex["answer"].strip()

    prompt = build_prompt(q)
    full_text = prompt + a + tokenizer.eos_token

    tokenized = tokenizer(full_text, truncation=True, max_length=MAX_LEN)
    prompt_ids = tokenizer(prompt, truncation=True, max_length=MAX_LEN)["input_ids"]

    labels = tokenized["input_ids"].copy()
    labels[:len(prompt_ids)] = [-100] * len(prompt_ids)

    tokenized["labels"] = labels
    return tokenized

tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,
    pad_to_multiple_of=8
)

training_args = TrainingArguments(
    output_dir=OUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    gradient_checkpointing=True,
    bf16=torch.cuda.is_bf16_supported(),
    fp16=not torch.cuda.is_bf16_supported(),
    optim="paged_adamw_8bit",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train()

model.save_pretrained(OUT_DIR)
tokenizer.save_pretrained(OUT_DIR)
print("‚úÖ Student LoRA kaydedildi")

!pip install -U bitsandbytes transformers accelerate peft datasets

import os, torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
DATA_PATH = os.path.join(BASE_PATH, "distill_Cosmos_T1_v3.jsonl")
OUT_DIR   = os.path.join(BASE_PATH, "adapter_qwen_student")

STUDENT_MODEL = "Metin/gemma-2b-tr-inst"

dataset = load_dataset("json", data_files=DATA_PATH, split="train")

tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"   # üî• Qwen i√ßin √∂nemli

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","mlp.up_proj","mlp.down_proj","mlp.gate_proj"]
)

model = get_peft_model(model, lora_config)

MAX_LEN = 1024

def build_prompt(question):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è √ñrneƒüi kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def preprocess(ex):
    q = ex["question"].strip()
    a = ex["answer"].strip()

    prompt = build_prompt(q)
    full_text = prompt + a + tokenizer.eos_token

    tokenized = tokenizer(full_text, truncation=True, max_length=MAX_LEN)
    prompt_ids = tokenizer(prompt, truncation=True, max_length=MAX_LEN)["input_ids"]

    labels = tokenized["input_ids"].copy()
    labels[:len(prompt_ids)] = [-100] * len(prompt_ids)

    tokenized["labels"] = labels
    return tokenized

tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True,
    pad_to_multiple_of=8
)

training_args = TrainingArguments(
    output_dir=OUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    gradient_checkpointing=True,
    bf16=torch.cuda.is_bf16_supported(),
    fp16=not torch.cuda.is_bf16_supported(),
    optim="paged_adamw_8bit",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train()

model.save_pretrained(OUT_DIR)
tokenizer.save_pretrained(OUT_DIR)
print("‚úÖ Student LoRA kaydedildi")

# ==========================================
# STUDENT TRAIN (LoRA SFT) - FINAL (Teacher-aligned + VRAM Friendly)
# Dynamic Padding + Correct Collator
# ==========================================
import os, torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
TRAIN_PATH = os.path.join(BASE_PATH, "distill_Cosmos_Llama_Guncel.jsonl")

STUDENT_MODEL = "Metin/gemma-2b-tr-inst"
OUT_ADAPTER_DIR = os.path.join(BASE_PATH, "adapter_gemma2b_LLama_dynamicpad")

# =========================
# TRAIN PARAMS
# =========================
MAX_LEN = 1024
BATCH_SIZE = 2
GRAD_ACC = 8
LR = 2e-4
EPOCHS = 2
WARMUP_RATIO = 0.03
LOG_STEPS = 10
SAVE_STEPS = 200

# =========================
# TEACHER ƒ∞LE AYNI PROMPT (D√úZ METƒ∞N)
# =========================
def build_prompt(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece formatƒ± g√∂stermek i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama! Kendi bulduƒüun sonucu yaz!\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

# =========================
# LOAD DATASET
# =========================
ds = load_dataset("json", data_files=TRAIN_PATH, split="train")
print("‚úÖ Train size:", len(ds))

# =========================
# TOKENIZER
# =========================
tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
tokenizer.padding_side = "right"   # train i√ßin doƒüru
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# =========================
# MODEL (4bit) & LoRA
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

base_model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

base_model.config.use_cache = False  # gradient checkpointing i√ßin ≈üart
base_model = prepare_model_for_kbit_training(base_model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
)

model = get_peft_model(base_model, lora_config)

# ‚úÖ EKLENDƒ∞: padding id uyumu
model.config.pad_token_id = tokenizer.pad_token_id

model.print_trainable_parameters()

# =========================
# PREPROCESS (Dynamic padding i√ßin padding yok!)
# =========================
def preprocess(ex):
    q = str(ex["question"]).strip()
    a = str(ex["answer"]).strip()

    prompt = build_prompt(q)

    # Teacher output formatƒ± zaten:
    # <think> ... </think>\nFinal Answer: ...
    full_text = prompt + a + tokenizer.eos_token

    tok_full = tokenizer(
        full_text,
        truncation=True,
        max_length=MAX_LEN,
        padding=False,
        add_special_tokens=False
    )

    tok_prompt = tokenizer(
        prompt,
        truncation=True,
        max_length=MAX_LEN,
        padding=False,
        add_special_tokens=False
    )

    input_ids = tok_full["input_ids"]
    attention_mask = tok_full["attention_mask"]

    labels = input_ids.copy()

    # Prompt kƒ±smƒ±nƒ± loss'tan √ßƒ±kar
    prompt_len = len(tok_prompt["input_ids"])
    mask_len = min(prompt_len, len(labels))
    labels[:mask_len] = [-100] * mask_len

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

ds_tok = ds.map(preprocess, remove_columns=ds.column_names)

# =========================
# COLLATOR (DOƒûRU OLAN)
# Dynamic padding + label padding = -100
# =========================
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    pad_to_multiple_of=8,
    label_pad_token_id=-100,
    return_tensors="pt"
)

# =========================
# TRAIN ARGS
# =========================
training_args = TrainingArguments(
    output_dir=OUT_ADAPTER_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACC,
    learning_rate=LR,
    num_train_epochs=EPOCHS,
    warmup_ratio=WARMUP_RATIO,
    logging_steps=LOG_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=2,
    gradient_checkpointing=True,   # üî• VRAM azaltƒ±r
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    optim="paged_adamw_8bit",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds_tok,
    data_collator=data_collator
)

# =========================
# TRAIN
# =========================
trainer.train()

# =========================
# SAVE
# =========================
model.save_pretrained(OUT_ADAPTER_DIR)
tokenizer.save_pretrained(OUT_ADAPTER_DIR)

print("‚úÖ Training bitti!")
print("üìå Adapter kaydedildi:", OUT_ADAPTER_DIR)

import os, json, re, torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"

TEST_PATH = os.path.join(BASE_PATH, "test_500_final.jsonl")
ADAPTER_DIR = os.path.join(BASE_PATH, "adapter_gemma2b_LLama_dynamicpad")

STUDENT_MODEL = "Metin/gemma-2b-tr-inst"
OUT_PRED_PATH = os.path.join(BASE_PATH, "eval_results_test500.jsonl")

MAX_NEW_TOKENS = 512
RETRY_TOKENS = 128

# =========================
# PROMPT (TRAIN ƒ∞LE Bƒ∞REBƒ∞R)
# =========================
def build_prompt(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece format i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def build_retry_prompt(question: str):
    return (
        f"Soru: {question}\n\n"
        "SADECE ≈üu formatta cevap ver:\n"
        "<think>\n...\n</think>\n"
        "Final Answer: <sayƒ±>\n"
        "Ba≈üka hi√ßbir ≈üey yazma."
    )

# =========================
# NUMBER EXTRACTION
# =========================
def safe_float(x):
    try:
        return float(str(x).strip().replace(",", "."))
    except:
        return None

def extract_final_number(text: str):
    m = re.search(r"(Final\s*Answer|Cevap|Sonu√ß)\s*:?\s*(.*)", text, flags=re.IGNORECASE)
    if m:
        tail = m.group(2)
        m2 = re.search(r"([-+]?\d+(?:[.,]\d+)?)", tail)
        if m2:
            return m2.group(1)
    return None

def fallback_last_number(text: str):
    nums = re.findall(r"([-+]?\d+(?:[.,]\d+)?)", text)
    return nums[-1] if nums else None

def get_final_number(text):
    n = extract_final_number(text)
    if n is None:
        n = fallback_last_number(text)
    return n

# =========================
# MODEL LOAD
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

print("‚è≥ Tokenizer y√ºkleniyor...")
tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
tokenizer.padding_side = "left"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("‚è≥ Base model y√ºkleniyor...")
base_model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

print("‚è≥ Adapter y√ºkleniyor...")
model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
model.eval()
model.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Model hazƒ±r")

# =========================
# LOAD TEST DATA
# =========================
test_ds = load_dataset("json", data_files=TEST_PATH, split="train")
print("‚úÖ Test size:", len(test_ds))

# =========================
# EVAL LOOP
# =========================
correct = 0
total = 0
retried = 0
invalid_gt = 0
invalid_pred = 0

with open(OUT_PRED_PATH, "w", encoding="utf-8") as fout:
    for i, ex in enumerate(tqdm(test_ds)):
        q = ex["question"]
        gt_text = ex["answer"]

        gt_num = safe_float(get_final_number(gt_text))
        if gt_num is None:
            invalid_gt += 1
            continue

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        gen = out[0][inputs["input_ids"].shape[1]:]
        pred_text = tokenizer.decode(gen, skip_special_tokens=True).strip()

        pred_num = safe_float(get_final_number(pred_text))

        # Retry if needed
        did_retry = False
        if pred_num is None:
            did_retry = True
            retried += 1

            retry_prompt = build_retry_prompt(q)
            inp2 = tokenizer(retry_prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                out2 = model.generate(
                    **inp2,
                    max_new_tokens=RETRY_TOKENS,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            gen2 = out2[0][inp2["input_ids"].shape[1]:]
            retry_text = tokenizer.decode(gen2, skip_special_tokens=True).strip()

            pred_text += "\n\n[RETRY]\n" + retry_text
            pred_num = safe_float(get_final_number(retry_text))

        if pred_num is None:
            invalid_pred += 1

        is_correct = pred_num is not None and abs(pred_num - gt_num) < 1e-4

        total += 1
        correct += int(is_correct)

        fout.write(json.dumps({
            "idx": i,
            "question": q,
            "gt_num": gt_num,
            "pred_num": pred_num,
            "correct": is_correct,
            "retried": did_retry,
            "pred_text": pred_text
        }, ensure_ascii=False) + "\n")

# =========================
# METRICS
# =========================
acc = correct / total * 100 if total else 0

print("\nüìä RESULTS")
print(f"Accuracy: %{acc:.2f}")
print(f"Correct: {correct}/{total}")
print(f"Retry used: {retried}")
print(f"Invalid GT: {invalid_gt}")
print(f"Invalid Pred: {invalid_pred}")
print(f"Saved to: {OUT_PRED_PATH}")

!pip install -U torch torchvision torchaudio

# ==========================================
# STUDENT TRAIN (LoRA SFT) - FINAL (Teacher-aligned + VRAM Friendly)
# Dynamic Padding + Correct Collator
# ==========================================
import os, torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
TRAIN_PATH = os.path.join(BASE_PATH, "distill_Cosmos_T1_v3_guncel.jsonl")

STUDENT_MODEL = "Metin/gemma-2b-tr-inst"
OUT_ADAPTER_DIR = os.path.join(BASE_PATH, "adapter_gemma2b_CosmosT1v3_dynamicpad")

# =========================
# TRAIN PARAMS
# =========================
MAX_LEN = 1024
BATCH_SIZE = 2
GRAD_ACC = 8
LR = 2e-4
EPOCHS = 2
WARMUP_RATIO = 0.03
LOG_STEPS = 10
SAVE_STEPS = 200

# =========================
# TEACHER ƒ∞LE AYNI PROMPT (D√úZ METƒ∞N)
# =========================
def build_prompt(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece formatƒ± g√∂stermek i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama! Kendi bulduƒüun sonucu yaz!\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

# =========================
# LOAD DATASET
# =========================
ds = load_dataset("json", data_files=TRAIN_PATH, split="train")
print("‚úÖ Train size:", len(ds))

# =========================
# TOKENIZER
# =========================
tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
tokenizer.padding_side = "right"   # train i√ßin doƒüru
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# =========================
# MODEL (4bit) & LoRA
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

base_model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

base_model.config.use_cache = False  # gradient checkpointing i√ßin ≈üart
base_model = prepare_model_for_kbit_training(base_model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
)

model = get_peft_model(base_model, lora_config)

# ‚úÖ EKLENDƒ∞: padding id uyumu
model.config.pad_token_id = tokenizer.pad_token_id

model.print_trainable_parameters()

# =========================
# PREPROCESS (Dynamic padding i√ßin padding yok!)
# =========================
def preprocess(ex):
    q = str(ex["question"]).strip()
    a = str(ex["answer"]).strip()

    prompt = build_prompt(q)

    # Teacher output formatƒ± zaten:
    # <think> ... </think>\nFinal Answer: ...
    full_text = prompt + a + tokenizer.eos_token

    tok_full = tokenizer(
        full_text,
        truncation=True,
        max_length=MAX_LEN,
        padding=False,
        add_special_tokens=False
    )

    tok_prompt = tokenizer(
        prompt,
        truncation=True,
        max_length=MAX_LEN,
        padding=False,
        add_special_tokens=False
    )

    input_ids = tok_full["input_ids"]
    attention_mask = tok_full["attention_mask"]

    labels = input_ids.copy()

    # Prompt kƒ±smƒ±nƒ± loss'tan √ßƒ±kar
    prompt_len = len(tok_prompt["input_ids"])
    mask_len = min(prompt_len, len(labels))
    labels[:mask_len] = [-100] * mask_len

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

ds_tok = ds.map(preprocess, remove_columns=ds.column_names)

# =========================
# COLLATOR (DOƒûRU OLAN)
# Dynamic padding + label padding = -100
# =========================
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    pad_to_multiple_of=8,
    label_pad_token_id=-100,
    return_tensors="pt"
)

# =========================
# TRAIN ARGS
# =========================
training_args = TrainingArguments(
    output_dir=OUT_ADAPTER_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACC,
    learning_rate=LR,
    num_train_epochs=EPOCHS,
    warmup_ratio=WARMUP_RATIO,
    logging_steps=LOG_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=2,
    gradient_checkpointing=True,   # üî• VRAM azaltƒ±r
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    optim="paged_adamw_8bit",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds_tok,
    data_collator=data_collator
)

# =========================
# TRAIN
# =========================
trainer.train()

# =========================
# SAVE
# =========================
model.save_pretrained(OUT_ADAPTER_DIR)
tokenizer.save_pretrained(OUT_ADAPTER_DIR)

print("‚úÖ Training bitti!")
print("üìå Adapter kaydedildi:", OUT_ADAPTER_DIR)

import os, json, re, torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"

TEST_PATH = os.path.join(BASE_PATH, "test_500_final.jsonl")
ADAPTER_DIR = os.path.join(BASE_PATH, "adapter_gemma2b_CosmosT1v3_dynamicpad")

STUDENT_MODEL = "Metin/gemma-2b-tr-inst"
OUT_PRED_PATH = os.path.join(BASE_PATH, "eval_results_test500.jsonl")

MAX_NEW_TOKENS = 512
RETRY_TOKENS = 128

# =========================
# PROMPT (TRAIN ƒ∞LE Bƒ∞REBƒ∞R)
# =========================
def build_prompt(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece format i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def build_retry_prompt(question: str):
    return (
        f"Soru: {question}\n\n"
        "SADECE ≈üu formatta cevap ver:\n"
        "<think>\n...\n</think>\n"
        "Final Answer: <sayƒ±>\n"
        "Ba≈üka hi√ßbir ≈üey yazma."
    )

# =========================
# NUMBER EXTRACTION
# =========================
def safe_float(x):
    try:
        return float(str(x).strip().replace(",", "."))
    except:
        return None

def extract_final_number(text: str):
    m = re.search(r"(Final\s*Answer|Cevap|Sonu√ß)\s*:?\s*(.*)", text, flags=re.IGNORECASE)
    if m:
        tail = m.group(2)
        m2 = re.search(r"([-+]?\d+(?:[.,]\d+)?)", tail)
        if m2:
            return m2.group(1)
    return None

def fallback_last_number(text: str):
    nums = re.findall(r"([-+]?\d+(?:[.,]\d+)?)", text)
    return nums[-1] if nums else None

def get_final_number(text):
    n = extract_final_number(text)
    if n is None:
        n = fallback_last_number(text)
    return n

# =========================
# MODEL LOAD
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

print("‚è≥ Tokenizer y√ºkleniyor...")
tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)
tokenizer.padding_side = "left"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("‚è≥ Base model y√ºkleniyor...")
base_model = AutoModelForCausalLM.from_pretrained(
    STUDENT_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

print("‚è≥ Adapter y√ºkleniyor...")
model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
model.eval()
model.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Model hazƒ±r")

# =========================
# LOAD TEST DATA
# =========================
test_ds = load_dataset("json", data_files=TEST_PATH, split="train")
print("‚úÖ Test size:", len(test_ds))

# =========================
# EVAL LOOP
# =========================
correct = 0
total = 0
retried = 0
invalid_gt = 0
invalid_pred = 0

with open(OUT_PRED_PATH, "w", encoding="utf-8") as fout:
    for i, ex in enumerate(tqdm(test_ds)):
        q = ex["question"]
        gt_text = ex["answer"]

        gt_num = safe_float(get_final_number(gt_text))
        if gt_num is None:
            invalid_gt += 1
            continue

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        gen = out[0][inputs["input_ids"].shape[1]:]
        pred_text = tokenizer.decode(gen, skip_special_tokens=True).strip()

        pred_num = safe_float(get_final_number(pred_text))

        # Retry if needed
        did_retry = False
        if pred_num is None:
            did_retry = True
            retried += 1

            retry_prompt = build_retry_prompt(q)
            inp2 = tokenizer(retry_prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                out2 = model.generate(
                    **inp2,
                    max_new_tokens=RETRY_TOKENS,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            gen2 = out2[0][inp2["input_ids"].shape[1]:]
            retry_text = tokenizer.decode(gen2, skip_special_tokens=True).strip()

            pred_text += "\n\n[RETRY]\n" + retry_text
            pred_num = safe_float(get_final_number(retry_text))

        if pred_num is None:
            invalid_pred += 1

        is_correct = pred_num is not None and abs(pred_num - gt_num) < 1e-4

        total += 1
        correct += int(is_correct)

        fout.write(json.dumps({
            "idx": i,
            "question": q,
            "gt_num": gt_num,
            "pred_num": pred_num,
            "correct": is_correct,
            "retried": did_retry,
            "pred_text": pred_text
        }, ensure_ascii=False) + "\n")

# =========================
# METRICS
# =========================
acc = correct / total * 100 if total else 0

print("\nüìä RESULTS")
print(f"Accuracy: %{acc:.2f}")
print(f"Correct: {correct}/{total}")
print(f"Retry used: {retried}")
print(f"Invalid GT: {invalid_gt}")
print(f"Invalid Pred: {invalid_pred}")
print(f"Saved to: {OUT_PRED_PATH}")

import os, json, re, torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from tqdm import tqdm

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"

TEST_PATH = os.path.join(BASE_PATH, "test_500_final.jsonl")

# üëâ BURAYA TEACHER MODEL ADINI YAZ
TEACHER_MODEL = "ytu-ce-cosmos/Turkish-Llama-8b-Instruct-v0.1"

OUT_PRED_PATH = os.path.join(BASE_PATH, "eval_results_cosmost1teacher_test500.jsonl")

MAX_NEW_TOKENS = 512
RETRY_TOKENS = 128

# =========================
# PROMPT (TRAIN ƒ∞LE Bƒ∞REBƒ∞R)
# =========================
def build_prompt(question: str):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )

    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. ‚ö†Ô∏è Dƒ∞KKAT: A≈üaƒüƒ±daki √∂rnek sadece format i√ßindir. √ñrnekteki sayƒ±larƒ± ASLA kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def build_retry_prompt(question: str):
    return (
        f"Soru: {question}\n\n"
        "SADECE ≈üu formatta cevap ver:\n"
        "<think>\n...\n</think>\n"
        "Final Answer: <sayƒ±>\n"
        "Ba≈üka hi√ßbir ≈üey yazma."
    )

# =========================
# NUMBER EXTRACTION
# =========================
def safe_float(x):
    try:
        return float(str(x).strip().replace(",", "."))
    except:
        return None

def extract_final_number(text: str):
    m = re.search(r"(Final\s*Answer|Cevap|Sonu√ß)\s*:?\s*(.*)", text, flags=re.IGNORECASE)
    if m:
        tail = m.group(2)
        m2 = re.search(r"([-+]?\d+(?:[.,]\d+)?)", tail)
        if m2:
            return m2.group(1)
    return None

def fallback_last_number(text: str):
    nums = re.findall(r"([-+]?\d+(?:[.,]\d+)?)", text)
    return nums[-1] if nums else None

def get_final_number(text):
    n = extract_final_number(text)
    if n is None:
        n = fallback_last_number(text)
    return n

# =========================
# MODEL LOAD (TEACHER)
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

print("‚è≥ Tokenizer y√ºkleniyor...")
tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL, use_fast=True)
tokenizer.padding_side = "left"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("‚è≥ Teacher model y√ºkleniyor...")
model = AutoModelForCausalLM.from_pretrained(
    TEACHER_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

model.eval()
model.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Teacher model hazƒ±r")

# =========================
# LOAD TEST DATA
# =========================
test_ds = load_dataset("json", data_files=TEST_PATH, split="train")
print("‚úÖ Test size:", len(test_ds))

# =========================
# EVAL LOOP
# =========================
correct = 0
total = 0
retried = 0
invalid_gt = 0
invalid_pred = 0

with open(OUT_PRED_PATH, "w", encoding="utf-8") as fout:
    for i, ex in enumerate(tqdm(test_ds)):
        q = ex["question"]
        gt_text = ex["answer"]

        gt_num = safe_float(get_final_number(gt_text))
        if gt_num is None:
            invalid_gt += 1
            continue

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        gen = out[0][inputs["input_ids"].shape[1]:]
        pred_text = tokenizer.decode(gen, skip_special_tokens=True).strip()

        pred_num = safe_float(get_final_number(pred_text))

        # Retry if needed
        did_retry = False
        if pred_num is None:
            did_retry = True
            retried += 1

            retry_prompt = build_retry_prompt(q)
            inp2 = tokenizer(retry_prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                out2 = model.generate(
                    **inp2,
                    max_new_tokens=RETRY_TOKENS,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            gen2 = out2[0][inp2["input_ids"].shape[1]:]
            retry_text = tokenizer.decode(gen2, skip_special_tokens=True).strip()

            pred_text += "\n\n[RETRY]\n" + retry_text
            pred_num = safe_float(get_final_number(retry_text))

        if pred_num is None:
            invalid_pred += 1

        is_correct = pred_num is not None and abs(pred_num - gt_num) < 1e-4

        total += 1
        correct += int(is_correct)

        fout.write(json.dumps({
            "idx": i,
            "question": q,
            "gt_num": gt_num,
            "pred_num": pred_num,
            "correct": is_correct,
            "retried": did_retry,
            "pred_text": pred_text
        }, ensure_ascii=False) + "\n")

# =========================
# METRICS
# =========================
acc = correct / total * 100 if total else 0

print("\nüìä RESULTS")
print(f"Accuracy: %{acc:.2f}")
print(f"Correct: {correct}/{total}")
print(f"Retry used: {retried}")
print(f"Invalid GT: {invalid_gt}")
print(f"Invalid Pred: {invalid_pred}")
print(f"Saved to: {OUT_PRED_PATH}")

!pip install -U transformers huggingface_hub accelerate datasets bitsandbytes peft

import os, json, re, torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from tqdm import tqdm

# =========================
# PATHS
# =========================
BASE_PATH = "/content/drive/MyDrive/Proje_Distill"
TEST_PATH = os.path.join(BASE_PATH, "test_500_final.jsonl")

TEACHER_MODEL = "hosasmek/qwen2.5-7b-instruct-reasoning-tr"
OUT_PRED_PATH = os.path.join(BASE_PATH, "eval_results_qwenteacher_test500.jsonl")

MAX_NEW_TOKENS = 512
RETRY_TOKENS   = 64

# =========================
# NUMBER UTILS (STUDENT ƒ∞LE AYNI)
# =========================
def safe_float(x):
    try:
        if x is None:
            return None
        return float(str(x).strip().replace(",", "."))
    except:
        return None

def extract_final_number(text):
    if not text:
        return None
    m = re.search(r"(?:Final\s*Answer|Cevap|Sonu√ß)\s*:?\s*(.*)", text, re.IGNORECASE)
    if m:
        tail = m.group(1)
        m2 = re.search(r"([-+]?\d+(?:[.,]\d+)?)", tail)
        if m2:
            return m2.group(1)
    return None

def fallback_last_number(text):
    nums = re.findall(r"([-+]?\d+(?:[.,]\d+)?)", str(text))
    return nums[-1] if nums else None

def get_final_number(text):
    n = extract_final_number(text)
    if n is None:
        n = fallback_last_number(text)
    return n

# =========================
# PROMPTS
# =========================
def build_prompt(question):
    example_output = (
        "<think>\n"
        "Alan = Uzunluk x Geni≈ülik\n"
        "Alan = 10 * 5 = 50\n"
        "</think>\n"
        "Final Answer: 50"
    )
    return (
        f"Soru: {question}\n\n"
        "G√ñREV: Adƒ±m adƒ±m d√º≈ü√ºn ve soruyu √ß√∂z.\n"
        "KURALLAR:\n"
        "1. √ñnce <think> etiketleri arasƒ±nda i≈ülemleri yap.\n"
        "2. Bulduƒüun sonucu en sona 'Final Answer: <sayƒ±>' formatƒ±nda yaz.\n"
        "3. √ñrnek sadece format i√ßindir, sayƒ±larƒ± kopyalama.\n\n"
        "√ñRNEK FORMAT:\n"
        f"{example_output}\n\n"
    )

def build_retry_prompt(question):
    return (
        f"Soru: {question}\n\n"
        "SADECE ≈üu formatta cevap ver:\n"
        "<think>\n...\n</think>\n"
        "Final Answer: <sayƒ±>"
    )

# =========================
# MODEL LOAD
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

print("‚è≥ Tokenizer y√ºkleniyor...")
tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL, use_fast=True)
tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token

print("‚è≥ Teacher model y√ºkleniyor...")
model = AutoModelForCausalLM.from_pretrained(
    TEACHER_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)
model.eval()
model.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Teacher model hazƒ±r")

# =========================
# LOAD TEST DATA
# =========================
test_ds = load_dataset("json", data_files=TEST_PATH, split="train")
print("‚úÖ Test size:", len(test_ds))

# =========================
# üîÅ RESUME LOGIC (YENƒ∞)
# =========================
start_index = 0
if os.path.exists(OUT_PRED_PATH):
    with open(OUT_PRED_PATH, "r", encoding="utf-8") as f:
        start_index = sum(1 for _ in f)

print(f"üîÅ Resume from index: {start_index}")

# =========================
# EVAL LOOP
# =========================
correct = total = invalid_gt = invalid_pred = retried = 0

with open(OUT_PRED_PATH, "a", encoding="utf-8") as fout:
    for i in tqdm(range(start_index, len(test_ds))):
        ex = test_ds[i]
        q = ex["question"]
        gt_text = ex["answer"]

        gt_num = safe_float(get_final_number(gt_text))
        if gt_num is None:
            invalid_gt += 1
            continue

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.inference_mode():
            out = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        gen = out[0][inputs["input_ids"].shape[1]:]
        pred_text = tokenizer.decode(gen, skip_special_tokens=True).strip()
        pred_num = safe_float(get_final_number(pred_text))

        did_retry = False
        if pred_num is None:
            did_retry = True
            retried += 1

            retry_prompt = build_retry_prompt(q)
            inp2 = tokenizer(retry_prompt, return_tensors="pt").to(model.device)

            with torch.inference_mode():
                out2 = model.generate(
                    **inp2,
                    max_new_tokens=RETRY_TOKENS,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            gen2 = out2[0][inp2["input_ids"].shape[1]:]
            retry_text = tokenizer.decode(gen2, skip_special_tokens=True).strip()
            pred_num = safe_float(get_final_number(retry_text))

        if pred_num is None:
            invalid_pred += 1

        is_correct = pred_num is not None and abs(pred_num - gt_num) < 1e-4
        total += 1
        correct += int(is_correct)

        fout.write(json.dumps({
            "idx": i,
            "question": q,
            "gt_num": gt_num,
            "pred_num": pred_num,
            "correct": is_correct,
            "retried": did_retry
        }, ensure_ascii=False) + "\n")
        fout.flush()

# =========================
# METRICS
# =========================
acc = correct / total * 100 if total else 0
print(f"\n‚úÖ Accuracy: %{acc:.2f} ({correct}/{total})")
print(f"GT skip: {invalid_gt} | Pred invalid: {invalid_pred} | Retry: {retried}")
print(f"üìÅ Saved to: {OUT_PRED_PATH}")